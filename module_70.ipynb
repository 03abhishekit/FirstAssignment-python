{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc59e2c-e88e-492f-a561-5205542139f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "The mathematical formula for a linear Support Vector Machine (SVM) involves finding a hyperplane that best separates the\n",
    "data points of different classes. For a dataset with n features, the decision boundary is a hyperplane defined by the equation:\n",
    "\n",
    "𝑤 ⋅ 𝑥 + 𝑏 = 0\n",
    "where:\n",
    "w is the weight vector (coefficients of the hyperplane).\n",
    "x is the feature vector.\n",
    "b is the bias term.\n",
    "The goal is to find the values of w and b such that the margin, which is the distance between the hyperplane and the nearest \n",
    "data points from each class, is maximized.\n",
    "\n",
    "\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "The objective function of a linear SVM is to minimize the following:\n",
    "\n",
    "1/2 ∥ 𝑤 ∥2\n",
    "subject to the constraints:\n",
    "𝑦𝑖(𝑤 ⋅ 𝑥𝑖 + 𝑏) ≥ 1 for all 𝑖\n",
    "where:\n",
    "𝑦𝑖 is the class label for the \n",
    "i-th training example (𝑦𝑖 ∈{−1,1}is the feature vector for the 𝑖\n",
    "i-th training example.\n",
    "The term \n",
    "1/2 ∥ 𝑤 ∥2  represents the regularization term that controls the margin width. The constraints ensure that each training example \n",
    "is correctly classified with a margin of at least 1.\n",
    "\n",
    "\n",
    "                    \n",
    "Q3. What is the kernel trick in SVM?\n",
    "                       \n",
    "The kernel trick is a technique used in SVMs to handle non-linearly separable data by mapping the input \n",
    "features into a higher-dimensional space where a linear hyperplane can be used to separate the classes. \n",
    "This is done using a kernel function \n",
    "𝐾(𝑥𝑖,𝑥𝑗) that computes the dot product in the higher-dimensional space without explicitly transforming the data. \n",
    "Common kernel functions include:\n",
    "\n",
    "Linear kernel: \n",
    "𝐾(𝑥𝑖,𝑥𝑗) = 𝑥𝑖⋅𝑥𝑗\n",
    "\n",
    "Polynomial kernel: \n",
    "𝐾(𝑥𝑖,𝑥𝑗) =(𝑥𝑖⋅𝑥𝑗 + 𝑐)𝑑\n",
    "\n",
    "Radial basis function (RBF) or Gaussian kernel: \n",
    "𝐾(𝑥i,𝑥𝑗) = exp(−𝛾∥𝑥𝑖 −𝑥𝑗∥2)\n",
    "\n",
    " \n",
    " \n",
    "Q4. What is the role of support vectors in SVM? Explain with an example.\n",
    "Support vectors are the data points that lie closest to the decision boundary (hyperplane). \n",
    "These points are crucial because they define the position and orientation of the hyperplane. \n",
    "The margin of the SVM is determined by the distance of the support vectors from the hyperplane.\n",
    "\n",
    "For example, consider a binary classification problem with two classes, where the data points are distributed \n",
    "such that a linear hyperplane can separate them. The support vectors are the points from each class that are closest to the hyperplane.\n",
    "The SVM algorithm adjusts the hyperplane to maximize the margin while ensuring that the support vectors are correctly classified.\n",
    "\n",
    "\n",
    "                       \n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin, and Hard margin in SVM.\n",
    "\n",
    "Hyperplane:\n",
    "The hyperplane is the decision boundary that separates the data points of different classes.\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal planes are the planes parallel to the hyperplane and passing through the support vectors. \n",
    "The margin is the distance between these two planes.\n",
    "\n",
    "Hard Margin:\n",
    "A hard margin SVM requires that all data points are perfectly classified, which means there are no misclassifications.\n",
    "This is only feasible if the data is linearly separable.\n",
    "\n",
    "Soft Margin:\n",
    "A soft margin SVM allows some misclassifications to handle non-linearly separable data and avoid overfitting. \n",
    "The soft margin introduces slack variables to penalize misclassifications.\n",
    "\n",
    "Example graphs:\n",
    "\n",
    "\n",
    "\n",
    "Figure 1: Hard Margin SVM\n",
    "\n",
    "\n",
    "\n",
    "Figure 2: Soft Margin SVM\n",
    "\n",
    "                       \n",
    "                       \n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "                       \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # We take the first two features for easy visualization\n",
    "y = iris.target\n",
    "\n",
    "# Binary classification: consider only class 0 and class 1\n",
    "X = X[y != 2]\n",
    "y = y[y != 2]\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear SVM classifier\n",
    "svm = SVC(kernel='linear', C=1.0)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for the testing set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Compute the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Plot the decision boundaries\n",
    "def plot_decision_boundary(clf, X, y):\n",
    "    h = .02\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.xlabel('Sepal length')\n",
    "    plt.ylabel('Sepal width')\n",
    "    plt.title('SVM Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(svm, X_test, y_test)\n",
    "\n",
    "# Try different values of the regularization parameter C\n",
    "for C in [0.1, 1, 10, 100]:\n",
    "    svm = SVC(kernel='linear', C=C)\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred = svm.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy with C={C}: {accuracy:.2f}')\n",
    "                       \n",
    "                       \n",
    "Bonus Task: Implement a linear SVM classifier from scratch using Python and compare its performance with the scikit-learn implementation.\n",
    "Due to the complexity of implementing SVM from scratch, I will outline the steps here:\n",
    "\n",
    "Define the primal problem:\n",
    "\n",
    "min\n",
    "𝑤,𝑏1/2∥𝑤∥2\n",
    "subject to\n",
    "𝑦𝑖(𝑤⋅𝑥𝑖 + 𝑏)≥1 for all 𝑖 w,b\n",
    "\n",
    "Convert to dual problem:\n",
    "\n",
    "max\n",
    "⁡𝛼∑𝑖=1𝑛 𝛼𝑖 −1/2∑𝑖=1𝑛 ∑𝑗 =1𝑛 𝛼𝑖 𝛼𝑗𝑦𝑖 𝑦𝑗(\n",
    "𝑥\n",
    "𝑖\n",
    "⋅\n",
    "𝑥\n",
    "𝑗\n",
    ")\n",
    "α\n",
    "max\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " α \n",
    "i\n",
    "​\n",
    " − \n",
    "2\n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "n\n",
    "​\n",
    "  \n",
    "j=1\n",
    "∑\n",
    "n\n",
    "​\n",
    " α \n",
    "i\n",
    "​\n",
    " α \n",
    "j\n",
    "​\n",
    " y \n",
    "i\n",
    "​\n",
    " y \n",
    "j\n",
    "​\n",
    " (x \n",
    "i\n",
    "​\n",
    " ⋅x \n",
    "j\n",
    "​\n",
    " )\n",
    "subject to \n",
    "𝛼\n",
    "𝑖\n",
    "≥\n",
    "0\n",
    "α \n",
    "i\n",
    "​\n",
    " ≥0 and \n",
    "∑\n",
    "𝑖\n",
    "=\n",
    "1\n",
    "𝑛\n",
    "𝛼\n",
    "𝑖\n",
    "𝑦\n",
    "𝑖\n",
    "=\n",
    "0\n",
    "∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " α \n",
    "i\n",
    "​\n",
    " y \n",
    "i\n",
    "​\n",
    " =0\n",
    "\n",
    "Solve the dual problem using quadratic programming.\n",
    "\n",
    "Implementing the full solution involves numerical optimization which is quite complex and often done using specialized libraries. \n",
    "For simplicity, I recommend using cvxopt for solving the quadratic programming problem.\n",
    "\n",
    "However, the steps involve setting up the primal and dual formulations, deriving the support vectors, and constructing the decision function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
