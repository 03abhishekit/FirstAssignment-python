{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d46a6a-5e06-4392-9e6e-f3bebd9d70f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "In machine learning algorithms, particularly in Support Vector Machines (SVM), kernel functions are used to transform the input\n",
    "data into a higher-dimensional space where it is easier to separate the data linearly. Polynomial functions are one type of kernel function.\n",
    "\n",
    "Polynomial Kernel\n",
    "The polynomial kernel is defined as:\n",
    "ùêæ(ùë•ùëñ,ùë•ùëó) = (ùë•ùëñ ‚ãÖ ùë•ùëó + ùëê )ùëë\n",
    "where:\n",
    "ùë•ùëñ and ùë•ùëó are input feature vectors.\n",
    "c is a free parameter trading off the influence of higher-order versus lower-order terms.\n",
    "d is the degree of the polynomial.\n",
    "\n",
    "By using a polynomial kernel, the algorithm can fit more complex, non-linear decision boundaries.\n",
    "\n",
    "\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "Below is an example of implementing an SVM with a polynomial kernel using Scikit-learn:\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM with a polynomial kernel\n",
    "svc_poly = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the model\n",
    "svc_poly.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = svc_poly.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "\n",
    "In Support Vector Regression (SVR), the parameter epsilon (œµ) defines a margin of tolerance where no penalty is given to errors. \n",
    "Increasing the value of œµ results in a wider margin, which means more data points will fall within this margin and not be considered support vectors. \n",
    "Consequently, the number of support vectors decreases as œµ increases.\n",
    "\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of \n",
    "Support Vector Regression (SVR)?\n",
    "\n",
    "Kernel Function: Determines the shape of the hyperplane in the higher-dimensional space.\n",
    "Different kernel functions can model different types of relationships (e.g., linear, polynomial, RBF).\n",
    "\n",
    "C Parameter: Controls the trade-off between achieving a low training error and a low testing error (i.e., generalization).\n",
    "A smaller C value encourages a larger margin, while a larger C value aims to classify all training examples correctly by reducing the margin.\n",
    "\n",
    "Epsilon (œµ) Parameter: Defines the margin of tolerance where no penalty is given. Larger œµ values create a wider margin, which may result in fewer support vectors \n",
    "and a simpler model.\n",
    "\n",
    "Gamma Parameter: Only relevant for RBF and polynomial kernels. It defines the influence of a single training example.\n",
    "A low value means ‚Äòfar‚Äô influence, and a high value means ‚Äòclose‚Äô influence. Higher gamma values can lead to overfitting.\n",
    "\n",
    "\n",
    "\n",
    "Q5. Assignment\n",
    "Steps:\n",
    "Import the necessary libraries and load the dataset\n",
    "Split the dataset into training and testing sets\n",
    "Preprocess the data using any technique of your choice (e.g., scaling, normalization)\n",
    "Create an instance of the SVC classifier and train it on the training data\n",
    "Use the trained classifier to predict the labels of the testing data\n",
    "Evaluate the performance of the classifier using any metric of your choice (e.g., accuracy, precision, recall, F1-score)\n",
    "Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomizedSearchCV to improve its performance\n",
    "Train the tuned classifier on the entire dataset\n",
    "Save the trained classifier to a file for future use\n",
    "\n",
    "Implementation:\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import joblib\n",
    "\n",
    "# Load dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocess the data (scaling)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Create an instance of the SVC classifier and train it on the training data\n",
    "svc = SVC(kernel='linear', random_state=42)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained classifier to predict the labels of the testing data\n",
    "y_pred = svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the classifier\n",
    "print(\"Initial model performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Tune the hyperparameters of the SVC classifier using GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'poly', 'rbf'],\n",
    "    'gamma': ['scale', 'auto'],\n",
    "    'degree': [2, 3, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(SVC(), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters found by GridSearchCV:\")\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "# Train the tuned classifier on the entire dataset\n",
    "best_svc = grid_search.best_estimator_\n",
    "best_svc.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained classifier to a file for future use\n",
    "joblib.dump(best_svc, 'best_svc_model.pkl')\n",
    "\n",
    "# Use the tuned classifier to predict the labels of the testing data\n",
    "y_pred_tuned = best_svc.predict(X_test)\n",
    "\n",
    "# Evaluate the performance of the tuned classifier\n",
    "print(\"Tuned model performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_tuned):.2f}\")\n",
    "print(classification_report(y_test, y_pred_tuned))\n",
    "\n",
    "# Plot decision boundaries (optional, for two features)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_boundary(clf, X, y):\n",
    "    # We only take the first two features for easy visualization\n",
    "    X = X[:, :2]\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "# Plot decision boundaries for initial and tuned models\n",
    "plot_decision_boundary(svc, X_train, y_train)\n",
    "plot_decision_boundary(best_svc, X_train, y_train)\n",
    "\n",
    "\n",
    "In the above code:\n",
    "\n",
    "We load the Iris dataset and split it into training and testing sets.\n",
    "We preprocess the data using StandardScaler.\n",
    "We create an SVC classifier instance, train it on the training data, and evaluate its initial performance.\n",
    "We tune the hyperparameters using GridSearchCV to find the best model.\n",
    "We train the tuned model on the entire training dataset and save it using joblib.\n",
    "We evaluate the performance of the tuned model and plot the decision boundaries for visualization.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
