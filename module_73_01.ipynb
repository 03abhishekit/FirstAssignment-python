{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f8cadf7-2d6c-4845-b1ae-bae077047fdd",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "\n",
    "Gradient Boosting Regression is a machine learning technique used for regression tasks. It builds an ensemble of weak learners, typically decision trees, in a sequential manner. Each subsequent tree is trained to correct the errors made by the previous trees, thereby gradually improving the model's performance. The idea is to minimize the loss function by combining the predictions of these weak learners in a gradient descent fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ef6544-3b68-4b64-91e1-b081e25fe702",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy.\n",
    "\n",
    "Here is a simple implementation of Gradient Boosting Regression from scratch using Python and NumPy:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "        self.initial_prediction = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.initial_prediction = np.mean(y)\n",
    "        predictions = np.full(y.shape, self.initial_prediction)\n",
    "        \n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            tree = self._build_tree(X, residuals)\n",
    "            self.trees.append(tree)\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.full((X.shape[0],), self.initial_prediction)\n",
    "        for tree in self.trees:\n",
    "            predictions += self.learning_rate * tree.predict(X)\n",
    "        return predictions\n",
    "\n",
    "    def _build_tree(self, X, residuals):\n",
    "        from sklearn.tree import DecisionTreeRegressor\n",
    "        tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "        tree.fit(X, residuals)\n",
    "        return tree\n",
    "\n",
    "# Create a small dataset\n",
    "X = np.array([[1], [2], [3], [4], [5]])\n",
    "y = np.array([1.5, 2.0, 2.5, 3.5, 3.0])\n",
    "\n",
    "# Train the model\n",
    "gbr = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "gbr.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = gbr.predict(X)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab025f8b-7ced-4cac-953a-d5ec0b28eaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model.\n",
    "To find the best hyperparameters, you can use Grid Search. Here's an example using Grid Search with scikit-learn's GridSearchCV:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [1, 3, 5]\n",
    "}\n",
    "\n",
    "# Since we implemented the GradientBoostingRegressor from scratch, we will use a wrapper to make it compatible with GridSearchCV\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class GradientBoostingRegressorWrapper(BaseEstimator, RegressorMixin, GradientBoostingRegressor):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        super().__init__(n_estimators, learning_rate, max_depth)\n",
    "\n",
    "gbr_wrapper = GradientBoostingRegressorWrapper()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=gbr_wrapper, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "print(\"Best MSE: \", -grid_search.best_score_)\n",
    "\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "A weak learner in Gradient Boosting is typically a model that performs slightly better than random guessing.\n",
    "Decision trees with shallow depth (i.e., stumps) are commonly used as weak learners. The idea is that by combining many weak learners, \n",
    "the boosting algorithm can create a strong learner that has better performance.\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "The intuition behind Gradient Boosting is to build a model sequentially by adding new models that correct the errors made by the previous models.\n",
    "Each new model is trained to predict the residuals (errors) of the combined ensemble of previous models. \n",
    "By minimizing these residuals, the algorithm improves the overall prediction accuracy step by step.\n",
    "\n",
    "\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "\n",
    "Gradient Boosting builds an ensemble of weak learners as follows:\n",
    "\n",
    "Initialize the model with a constant value, typically the mean of the target variable.\n",
    "Iterate over a specified number of boosting rounds (n_estimators):\n",
    "Compute the residuals between the actual values and the current predictions.\n",
    "Train a weak learner (e.g., a decision tree) to predict these residuals.\n",
    "Update the model by adding the weak learner's predictions multiplied by the learning rate.\n",
    "Combine the predictions of all weak learners to form the final strong learner.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
