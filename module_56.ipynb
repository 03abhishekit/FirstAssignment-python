{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fa0c4b-c023-4bc1-9c87-0632af1fe277",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1: Min-Max Scaling\n",
    "Definition:\n",
    "Min-Max scaling, also known as normalization, is a technique used to scale data to a fixed range, \n",
    "typically [0, 1] or [-1, 1]. \n",
    "It linearly transforms the original data to the new range\n",
    ".Formula:\n",
    "X′=(X−Xmin)/(Xmax−Xmin) ×(max′−min′)+min′X\n",
    "where X′ is the scaled value, \n",
    "X is the original value, Xmin and Xmax are the minimum and maximum values of the original data,\n",
    "and min′and max′ are the desired range.\n",
    "Example:\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Example data\n",
    "data = np.array([[1, 5, 10, 15, 20]]).T\n",
    "\n",
    "# Applying Min-Max Scaling to range [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data:\\n\", scaled_data)\n",
    "\n",
    "\n",
    "Q2: Unit Vector Technique\n",
    "Definition:\n",
    "The Unit Vector technique scales each feature vector to have a unit norm (length of 1). \n",
    "This is useful when the direction of the data points is more important than their magnitude.\n",
    "Formula:\n",
    "X′= X / ∥X∥\n",
    "where ∥X∥ is the Euclidean norm of the vector X.\n",
    "\n",
    "Example:\n",
    "    \n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Example data\n",
    "data = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "\n",
    "# Applying Unit Vector Scaling\n",
    "scaled_data = normalize(data, norm='l2')\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data:\\n\", scaled_data)\n",
    "\n",
    "\n",
    "Q3: Principal Component Analysis (PCA)\n",
    "Definition:\n",
    "PCA is a dimensionality reduction technique that transforms the original data into a new set of orthogonal features called principal components.\n",
    "These components capture the maximum variance in the data.\n",
    "\n",
    "Example:\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Plotting the results\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA of Iris Dataset')\n",
    "plt.show()\n",
    "\n",
    "Q4: PCA and Feature Extraction\n",
    "Relationship:\n",
    "PCA can be used for feature extraction by transforming the original features into principal components that capture the most variance.\n",
    "Example:\n",
    "# Load dataset\n",
    "X = iris.data\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print(\"Original Shape:\", X.shape)\n",
    "print(\"Transformed Shape:\", X_pca.shape)\n",
    "\n",
    "\n",
    "Q5: Using Min-Max Scaling for a Food Delivery Recommendation System\n",
    "\n",
    "Steps:Load the dataset containing features like price, rating, and delivery time.\n",
    "Apply Min-Max Scaling to each feature to scale them to a fixed range.\n",
    "\n",
    "Example:\n",
    "# Example data\n",
    "data = np.array([[10, 4.5, 30], [15, 4.0, 25], [20, 4.8, 35], [25, 3.8, 40]])\n",
    "\n",
    "# Applying Min-Max Scaling to range [0, 1]\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data:\\n\", scaled_data)\n",
    "\n",
    "Q6: Using PCA to Reduce Dimensionality for Stock Price Prediction\n",
    "\n",
    "Steps:Load the dataset containing features like company financial data and market trends.\n",
    "Apply PCA to reduce the number of features while retaining the most important information\n",
    "\n",
    ".Example:pythonCopy code# Example data (assuming a large number of features)\n",
    "data = np.random.rand(100, 50)  # 100 samples, 50 features\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "print(\"Original Shape:\", data.shape)\n",
    "print(\"Transformed Shape:\", data_pca.shape)\n",
    "Q7: Min-Max Scaling to Range [-1, 1]Steps:Load the dataset containing the values [1, 5, 10, 15, 20].Apply Min-Max Scaling to transform the values to the range [-1, 1].Example:pythonCopy code# Data\n",
    "data = np.array([[1, 5, 10, 15, 20]]).T\n",
    "\n",
    "# Applying Min-Max Scaling to range [-1, 1]\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original Data:\\n\", data)\n",
    "print(\"Scaled Data:\\n\", scaled_data)\n",
    "Q8: Feature Extraction using PCASteps:Load the dataset containing features like height, weight, age, gender, and blood pressure.Apply PCA and determine the number of principal components to retain based on the explained variance.Example:pythonCopy code# Example data\n",
    "data = np.random.rand(100, 5)  # 100 samples, 5 features\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "data_pca = pca.fit_transform(data)\n",
    "\n",
    "# Determine the number of components to retain\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Choose the number of components that explain at least 95% of the variance\n",
    "n_components = np.argmax(cumulative_variance &gt;= 0.95) + 1\n",
    "\n",
    "print(\"Explained Variance Ratios:\", explained_variance)\n",
    "print(\"Cumulative Variance:\", cumulative_variance)\n",
    "print(\"Number of components to retain:\", n_components)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
