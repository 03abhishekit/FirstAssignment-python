{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9a4ad6-3b1c-4331-9c39-814896d1e610",
   "metadata": {},
   "source": [
    "Q1: What is anomaly detection and what is its purpose?\n",
    "\n",
    "Anomaly Detection is the process of identifying data points, events, or observations that deviate significantly from the majority of the data, which are considered normal. These deviations are called anomalies or outliers.\n",
    "\n",
    "Purpose:\n",
    "\n",
    "Fraud Detection: Identifying fraudulent activities in financial transactions.\n",
    "Network Security: Detecting intrusions or abnormal network traffic.\n",
    "Fault Detection: Identifying defects or malfunctions in industrial systems.\n",
    "Medical Diagnostics: Detecting rare diseases or abnormal health conditions from medical data.\n",
    "\n",
    "Q2: What are the key challenges in anomaly detection?\n",
    "\n",
    "Key Challenges:\n",
    "\n",
    "Imbalanced Data: Anomalies are rare compared to normal instances, making it difficult to learn patterns from them.\n",
    "High Dimensionality: Data with many features can obscure the distance between points, complicating the detection of anomalies.\n",
    "Noise: Differentiating between noise and actual anomalies can be challenging.\n",
    "Dynamic Data: Anomalies may change over time, requiring adaptive or real-time detection methods.\n",
    "Lack of Labeled Data: Supervised methods require labeled data, which is often unavailable for anomalies.\n",
    "\n",
    "\n",
    "Q3: How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "\n",
    "Unsupervised Anomaly Detection:\n",
    "\n",
    "No Labeled Data: Operates without labeled training data.\n",
    "Assumption: Anomalies are statistically different from the majority of data.\n",
    "Methods: Techniques include clustering, density estimation, and distance-based methods.\n",
    "Supervised Anomaly Detection:\n",
    "\n",
    "Labeled Data: Requires a labeled dataset with normal and anomalous examples.\n",
    "Learning Patterns: Learns to distinguish between normal and anomalous data based on labeled examples.\n",
    "Methods: Techniques include classification algorithms like SVM, neural networks, and decision trees.\n",
    "\n",
    "Q4: What are the main categories of anomaly detection algorithms?\n",
    "Main Categories:\n",
    "\n",
    "Statistical Methods: Assumes normal data follows a known distribution. Anomalies deviate significantly from this distribution.\n",
    "Examples: Z-score, Gaussian models.\n",
    "Proximity-Based Methods: Detect anomalies based on the distance or density of data points.\n",
    "Examples: k-NN, Local Outlier Factor (LOF).\n",
    "Clustering-Based Methods: Anomalies are points that do not fit well into any cluster.\n",
    "Examples: k-means, DBSCAN.\n",
    "Machine Learning Methods: Utilizes machine learning algorithms to detect anomalies.\n",
    "Examples: One-Class SVM, Isolation Forest.\n",
    "\n",
    "\n",
    "Q5: What are the main assumptions made by distance-based anomaly detection methods?\n",
    "\n",
    "Main Assumptions:\n",
    "\n",
    "Normal Points are Dense: Normal data points form dense regions.\n",
    "Anomalous Points are Sparse: Anomalies are far from the dense regions of normal data.\n",
    "Distance Metric: A suitable distance metric (e.g., Euclidean distance) effectively measures the similarity or dissimilarity between data points.\n",
    "\n",
    "Q6: How does the LOF algorithm compute anomaly scores?\n",
    "\n",
    "Local Outlier Factor (LOF) algorithm computes anomaly scores by:\n",
    "\n",
    "Local Density Estimation: For each data point, calculate the local density based on the distance to its k-nearest neighbors.\n",
    "Reachability Distance: The reachability distance of a point A with respect to point B is the maximum of the distance from A to B and the k-distance of \n",
    "B.\n",
    "Local Reachability Density (LRD): The inverse of the average reachability distance of a point to its k-nearest neighbors.\n",
    "LOF Score: The ratio of the average local reachability density of the point's neighbors to the point's own local reachability density. Higher scores indicate higher likelihood of being an anomaly.\n",
    "LOF(𝐴)=∑𝐵∈𝑘𝑁𝑁(𝐴)(LRD(𝐵) / LRD(𝐴)) / 𝑘\n",
    "\n",
    " \n",
    "Q7: What are the key parameters of the Isolation Forest algorithm?\n",
    "\n",
    "Key Parameters:\n",
    "\n",
    "Number of Trees (n_estimators): The number of trees in the forest.\n",
    "Subsampling Size (max_samples): The number of samples used to train each tree.\n",
    "Contamination: The proportion of anomalies in the dataset (if known), used to threshold anomaly scores.\n",
    "Random Seed (random_state): Controls the randomness of the sample selection and feature splits.\n",
    "\n",
    "\n",
    "Q8: If a data point has only 2 neighbors of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "In k-NN anomaly detection with K=10:\n",
    "\n",
    "Anomaly Score: Typically, the anomaly score is based on the distance to the K-th nearest neighbor or the proportion of neighbors within a certain distance.\n",
    "Given the point has only 2 neighbors within a radius of 0.5 (out of 10 required):\n",
    "\n",
    "Interpretation: The point likely has a high anomaly score because it does not have sufficient neighbors within the specified radius.\n",
    "\n",
    "\n",
    "Q9: Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?\n",
    "Isolation Forest:\n",
    "\n",
    "Average Path Length (h(x)): The average number of splits required to isolate a data point.\n",
    "Normalization Factor (c(n)): For n data points, the average path length of unsuccessful search in a Binary Search Tree.\n",
    "𝑐(𝑛)≈2𝐻(𝑛−1)−2(𝑛−1)𝑛\n",
    "where \n",
    "H(n) is the harmonic number, approximated by \n",
    "\n",
    "ln(n)+γ (Euler's constant).\n",
    "\n",
    "For \n",
    "n=3000:\n",
    "𝑐(3000)≈2(ln⁡(2999)+𝛾)−2(2999)3000\n",
    "𝑐(3000)≈2(8.006+0.577)−1.999\n",
    "c(3000)≈17.166−1.999≈15.167\n",
    "Anomaly Score:\n",
    "\n",
    "\n",
    "s(x,n)=2 \n",
    "− \n",
    "c(n)\n",
    "h(x)\n",
    "​\n",
    " \n",
    " \n",
    "Given \n",
    "ℎ\n",
    "(\n",
    "𝑥\n",
    ")\n",
    "=\n",
    "5.0\n",
    "h(x)=5.0:\n",
    "\n",
    "𝑠\n",
    "(\n",
    "𝑥\n",
    ",\n",
    "3000\n",
    ")\n",
    "=\n",
    "2\n",
    "−\n",
    "5.0\n",
    "15.167\n",
    "≈\n",
    "2\n",
    "−\n",
    "0.329\n",
    "≈\n",
    "0.80\n",
    "s(x,3000)=2 \n",
    "− \n",
    "15.167\n",
    "5.0\n",
    "​\n",
    " \n",
    " ≈2 \n",
    "−0.329\n",
    " ≈0.80\n",
    "The anomaly score for the data point is approximately 0.80, indicating it is not considered an extreme anomaly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
