{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4756345-4455-42f1-99ab-f212283c851a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random Forest Classifier for Predicting Heart Disease Risk\n",
    "\n",
    "\n",
    "Here is the step-by-step implementation:\n",
    "\n",
    "Preprocess the dataset by handling missing values, encoding categorical variables, and scaling numerical features.\n",
    "\n",
    "Split the dataset into training and testing sets.\n",
    "\n",
    "Train a Random Forest Classifier with default hyperparameters.\n",
    "\n",
    "Evaluate the model performance using accuracy, precision, recall, and F1 score.\n",
    "\n",
    "Identify and visualize the top 5 most important features.\n",
    "\n",
    "Tune the hyperparameters of the model using GridSearchCV.\n",
    "\n",
    "Compare the performance of the tuned model with the default model.\n",
    "\n",
    "Interpret the model by analyzing decision boundaries.\n",
    "\n",
    "Let's start implementing this in a Jupyter Notebook:\n",
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "url = \"https://drive.google.com/uc?export=download&id=1bGoIE4Z2kG5nyh-fGZAJ7LH0ki3UfmSJ\"\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# Preprocess the dataset\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoder = LabelEncoder()\n",
    "data_imputed['sex'] = label_encoder.fit_transform(data_imputed['sex'])\n",
    "data_imputed['cp'] = label_encoder.fit_transform(data_imputed['cp'])\n",
    "data_imputed['restecg'] = label_encoder.fit_transform(data_imputed['restecg'])\n",
    "data_imputed['slope'] = label_encoder.fit_transform(data_imputed['slope'])\n",
    "data_imputed['thal'] = label_encoder.fit_transform(data_imputed['thal'])\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n",
    "data_imputed[numerical_features] = scaler.fit_transform(data_imputed[numerical_features])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = data_imputed.drop('target', axis=1)\n",
    "y = data_imputed['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = clf.feature_importances_\n",
    "features = X.columns\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': feature_importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Visualize feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(5))\n",
    "plt.title('Top 5 Feature Importances')\n",
    "plt.show()\n",
    "\n",
    "# Hyperparameter tuning using GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best parameters: {grid_search.best_params_}')\n",
    "\n",
    "# Train the tuned model\n",
    "best_clf = grid_search.best_estimator_\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the tuned model\n",
    "y_pred_tuned = best_clf.predict(X_test)\n",
    "accuracy_tuned = accuracy_score(y_test, y_pred_tuned)\n",
    "precision_tuned = precision_score(y_test, y_pred_tuned)\n",
    "recall_tuned = recall_score(y_test, y_pred_tuned)\n",
    "f1_tuned = f1_score(y_test, y_pred_tuned)\n",
    "\n",
    "print(f'Tuned Accuracy: {accuracy_tuned:.2f}')\n",
    "print(f'Tuned Precision: {precision_tuned:.2f}')\n",
    "print(f'Tuned Recall: {recall_tuned:.2f}')\n",
    "print(f'Tuned F1 Score: {f1_tuned:.2f}')\n",
    "\n",
    "# Plotting decision boundaries for the two most important features\n",
    "top_features = importance_df['Feature'].head(2).values\n",
    "X_top_features = X[top_features]\n",
    "\n",
    "# Fit model on two features\n",
    "clf_top_features = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "clf_top_features.fit(X_train[top_features], y_train)\n",
    "\n",
    "# Plot decision boundaries\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=X_top_features.iloc[:, 0], y=X_top_features.iloc[:, 1], hue=y)\n",
    "plt.title('Decision Boundaries with Top 2 Features')\n",
    "plt.xlabel(top_features[0])\n",
    "plt.ylabel(top_features[1])\n",
    "plt.show()\n",
    "Explanation:\n",
    "Preprocessing:\n",
    "\n",
    "Handled missing values using mean imputation.\n",
    "Encoded categorical variables using LabelEncoder.\n",
    "Scaled numerical features using StandardScaler.\n",
    "Train-Test Split:\n",
    "\n",
    "Split the dataset into 70% training and 30% testing sets.\n",
    "Model Training:\n",
    "\n",
    "Trained a Random Forest Classifier with 100 trees and a maximum depth of 10.\n",
    "Model Evaluation:\n",
    "\n",
    "Evaluated the model using accuracy, precision, recall, and F1 score.\n",
    "Feature Importance:\n",
    "\n",
    "Identified and visualized the top 5 most important features.\n",
    "Hyperparameter Tuning:\n",
    "\n",
    "Tuned the model using GridSearchCV with a specified parameter grid.\n",
    "Reported the best parameters and performance metrics.\n",
    "\n",
    "Decision Boundaries:\n",
    "\n",
    "Analyzed the decision boundaries using the two most important features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
