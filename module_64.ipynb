{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986d44a-5e94-4ce9-ad40-bbb3a07ce6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "\n",
    "Ridge Regression:\n",
    "\n",
    "Concept: Ridge regression is a type of linear regression that includes a regularization term. It aims to prevent overfitting by penalizing large coefficients. The regularization term is the L2 norm of the coefficients.\n",
    "Cost Function:\n",
    "CostÂ Function = RSS + ğœ† âˆ‘ğ‘—=1ğ‘ ğ›½ğ‘—2\n",
    "\n",
    "Where \n",
    "\n",
    "Î» is the regularization parameter.\n",
    "\n",
    "Difference from OLS Regression:\n",
    "\n",
    "OLS Regression: Minimizes the residual sum of squares (RSS) to find the best-fitting line.\n",
    "\n",
    "Ridge Regression: Minimizes the residual sum of squares with an added penalty term (ğœ† âˆ‘ğ‘—=1ğ‘ ğ›½ğ‘—2) to shrink the coefficients, reducing the risk of overfitting.\n",
    "\n",
    "\n",
    "Q2. What are the assumptions of Ridge Regression?\n",
    "Assumptions:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables is linear.\n",
    "Independence: Observations are independent of each other.\n",
    "Homoscedasticity: Constant variance of the errors.\n",
    "No perfect multicollinearity: While Ridge Regression can handle multicollinearity better than OLS, perfect multicollinearity\n",
    "still needs to be avoided.\n",
    "Normality: The errors are normally distributed (particularly important for small sample sizes).\n",
    "\n",
    "\n",
    "Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "\n",
    "Selecting Lambda (Î»):\n",
    "\n",
    "Cross-Validation: The most common method is to use cross-validation to find the optimal value of ğœ†.This involves splitting the data into training\n",
    "and validation sets multiple times and selecting the Î» that minimizes the validation error.\n",
    "\n",
    "Grid Search: A grid search over a range of Î» values can be performed to identify the best value.\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Example using GridSearchCV to find the optimal lambda\n",
    "ridge = Ridge()\n",
    "params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(ridge, params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best lambda (alpha):\", grid_search.best_params_)\n",
    "\n",
    "\n",
    "Q4. Can Ridge Regression be used for feature selection? If yes, how?\n",
    "\n",
    "Ridge Regression for Feature Selection:\n",
    "\n",
    "Ridge Regression does not perform feature selection in the same way as Lasso Regression, which can shrink some coefficients to exactly zero. \n",
    "However, Ridge can still help in identifying important features by shrinking less important coefficients more than important ones.\n",
    "To use Ridge Regression for feature selection, you might follow up with techniques like Recursive Feature Elimination (RFE).\n",
    "\n",
    "Q5. How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "\n",
    "Handling Multicollinearity:\n",
    "\n",
    "Performance: Ridge Regression performs well in the presence of multicollinearity because the regularization term (\n",
    "ğœ†\n",
    "âˆ‘\n",
    "ğ‘—\n",
    "=\n",
    "1\n",
    "ğ‘\n",
    "ğ›½\n",
    "ğ‘—\n",
    "2\n",
    "Î»âˆ‘ \n",
    "j=1\n",
    "p\n",
    "â€‹\n",
    " Î² \n",
    "j\n",
    "2\n",
    "â€‹\n",
    " ) helps to shrink the coefficients, reducing the variance of the estimates.\n",
    "Advantage: Unlike OLS, which can produce large variance estimates in the presence of multicollinearity, Ridge provides more stable and reliable estimates.\n",
    "Q6. Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "Handling Different Types of Variables:\n",
    "\n",
    "Yes: Ridge Regression can handle both categorical and continuous variables.\n",
    "Preprocessing Required: Categorical variables need to be encoded (e.g., one-hot encoding) before being used in the regression model.\n",
    "python\n",
    "Copy code\n",
    "# Example of encoding categorical variables\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoded_categorical = encoder.fit_transform(categorical_data)\n",
    "\n",
    "# Combine with continuous variables\n",
    "import numpy as np\n",
    "X = np.hstack((encoded_categorical.toarray(), continuous_data))\n",
    "Q7. How do you interpret the coefficients of Ridge Regression?\n",
    "Interpreting Coefficients:\n",
    "\n",
    "The coefficients in Ridge Regression represent the relationship between the predictors and the response variable, just like in OLS. However, they are shrunk towards zero by the penalty term.\n",
    "Magnitude: A smaller coefficient indicates a weaker relationship between the predictor and the response.\n",
    "Direction: The sign of the coefficient indicates the direction of the relationship.\n",
    "Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?\n",
    "Using Ridge Regression for Time-Series:\n",
    "\n",
    "Yes: Ridge Regression can be used for time-series data, but it requires proper preprocessing.\n",
    "Stationarity: Ensure that the time-series data is stationary.\n",
    "Feature Engineering: Create lag features to capture the temporal dependencies.\n",
    "python\n",
    "Copy code\n",
    "# Example of creating lag features\n",
    "df['lag_1'] = df['value'].shift(1)\n",
    "df['lag_2'] = df['value'].shift(2)\n",
    "\n",
    "# Drop rows with NaN values\n",
    "df = df.dropna()\n",
    "\n",
    "# Use Ridge Regression on the lagged features\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(df[['lag_1', 'lag_2']], df['value'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
