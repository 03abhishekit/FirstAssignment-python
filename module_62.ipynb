{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d29407-300b-4337-8627-f6210b3f468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "Simple Linear Regression:\n",
    "Simple linear regression is a statistical method that examines the linear relationship between two variables: \n",
    "    one independent variable (predictor) and \n",
    "    one dependent variable (response).\n",
    "    The relationship is represented by the equation:\n",
    "\n",
    "𝑦 = 𝛽0 + 𝛽1𝑥 + 𝜖\n",
    "\n",
    "Where:\n",
    "y is the dependent variable.\n",
    "x is the independent variable.\n",
    "𝛽0 is the y-intercept.\n",
    "𝛽1 is the slope of the line.\n",
    "ϵ is the error term.\n",
    "Example:\n",
    "Predicting a person's weight (dependent variable) based on their height (independent variable).\n",
    "\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression is an extension of simple linear regression that involves two or more independent variables to\n",
    "predict the dependent variable. The relationship is represented by the equation:\n",
    "\n",
    "𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + ⋯ +𝛽𝑛𝑥𝑛 + 𝜖\n",
    "\n",
    "Where:\n",
    "y is the dependent variable.\n",
    "𝑥1,𝑥2,…,𝑥𝑛  are the independent variables.\n",
    "𝛽0 is the y-intercept.\n",
    "𝛽1 , 𝛽2,…,𝛽𝑛  are the coefficients of the independent variables.\n",
    "ϵ is the error term.\n",
    "\n",
    "Example:\n",
    "Predicting a person's weight (dependent variable) based on their height, age, and gender (independent variables).\n",
    "\n",
    "Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "Assumptions of Linear Regression:\n",
    "\n",
    "Linearity: The relationship between the independent and dependent variables is linear.\n",
    "\n",
    "Check: Plot the dependent variable against each independent variable and look for a linear relationship.\n",
    "Independence: The observations are independent of each other.\n",
    "\n",
    "Check: Ensure the data collection process ensures independence (e.g., not time-series data unless properly accounted for).\n",
    "Homoscedasticity: The residuals (errors) have constant variance at all levels of the independent variables.\n",
    "\n",
    "Check: Plot the residuals versus fitted values and look for a random scatter pattern.\n",
    "Normality: The residuals of the model are normally distributed.\n",
    "\n",
    "Check: Use a Q-Q plot of the residuals or a Shapiro-Wilk test.\n",
    "No multicollinearity: The independent variables are not highly correlated with each other.\n",
    "\n",
    "Check: Calculate Variance Inflation Factor (VIF) for each independent variable.\n",
    "\n",
    "Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "Intercept (𝛽0): The expected value of the dependent variable when all independent variables are zero. It is the point where the regression line crosses the y-axis.\n",
    "\n",
    "Slope (𝛽1): The expected change in the dependent variable for a one-unit change in the independent variable, holding all other variables constant.\n",
    "\n",
    "Example:\n",
    "In a simple linear regression model predicting house prices based on square footage:\n",
    "Price = 50000 + 150 × Square Footage\n",
    "\n",
    "\n",
    "Intercept (𝛽0 = 50000): When the square footage is 0, the expected price of the house is $50,000.\n",
    "Slope (𝛽1 = 150): For each additional square foot, the expected price of the house increases by $150.\n",
    "\n",
    "Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
    "\n",
    "Gradient Descent:\n",
    "Gradient descent is an optimization algorithm used to minimize the cost function in machine learning models. \n",
    "The cost function measures how well the model's predictions match the actual data. \n",
    "Gradient descent iteratively adjusts the model parameters to find the minimum of the cost function.\n",
    "\n",
    "Steps in Gradient Descent:\n",
    "\n",
    "Initialize the parameters (e.g., weights) randomly.\n",
    "Compute the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "Update the parameters by moving in the direction opposite to the gradient, scaled by a learning rate.\n",
    "Repeat steps 2 and 3 until convergence (when the changes in the cost function are sufficiently small).\n",
    "𝜃: = 𝜃 − 𝛼⋅ ∇ 𝐽(𝜃)\n",
    "Where:\n",
    "θ represents the model parameters.\n",
    "α is the learning rate.\n",
    "∇J(θ) is the gradient of the cost function.\n",
    "\n",
    "\n",
    "Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "Multiple Linear Regression:\n",
    "Multiple linear regression involves predicting a dependent variable using multiple independent variables. The relationship is modeled as:\n",
    "\n",
    "𝑦 = 𝛽0 + 𝛽1𝑥1 + 𝛽2𝑥2 + ⋯ +𝛽𝑛𝑥𝑛 + 𝜖\n",
    "\n",
    "Differences from Simple Linear Regression:\n",
    "\n",
    "Number of Predictors: Simple linear regression uses one predictor, while multiple linear regression uses two or more.\n",
    "Model Complexity: Multiple linear regression can capture more complex relationships by considering multiple factors simultaneously.\n",
    "Interpretation: Each coefficient in multiple linear regression represents the effect of its corresponding predictor variable on the\n",
    "dependent variable, holding all other predictors constant.\n",
    "\n",
    "\n",
    "Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "Multicollinearity:\n",
    "Multicollinearity occurs when two or more independent variables in a multiple regression model are highly correlated,\n",
    "making it difficult to isolate the effect of each predictor on the dependent variable. This can lead to unstable estimates of \n",
    "regression coefficients and reduce the interpretability of the model.\n",
    "\n",
    "Detection:\n",
    "\n",
    "Variance Inflation Factor (VIF): A VIF value greater than 10 (or sometimes 5) indicates high multicollinearity.\n",
    "Correlation Matrix: Inspect the correlation matrix for high correlations between predictor variables.\n",
    "Addressing Multicollinearity:\n",
    "\n",
    "Remove Highly Correlated Predictors: Drop one of the correlated predictors.\n",
    "Principal Component Analysis (PCA): Transform the predictors into a set of uncorrelated components.\n",
    "Ridge Regression: Use regularization techniques that can handle multicollinearity by adding a penalty to the regression.\n",
    "\n",
    "\n",
    "Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "Polynomial Regression:\n",
    "Polynomial regression is a type of regression analysis where the relationship between the independent variable \n",
    "\n",
    "x and the dependent variable y is modeled as an nth-degree polynomial. The model is represented as:\n",
    "\n",
    "𝑦 = 𝛽0 + 𝛽1𝑥 + 𝛽2𝑥2 + ⋯ +𝛽𝑛𝑥𝑛 + 𝜖\n",
    "y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " x+β \n",
    "2\n",
    "​\n",
    " x \n",
    "2\n",
    " +⋯+β \n",
    "n\n",
    "​\n",
    " x \n",
    "n\n",
    " +ϵ\n",
    "\n",
    "Differences from Linear Regression:\n",
    "\n",
    "Nature of Relationship: Linear regression assumes a linear relationship between \n",
    "𝑥\n",
    "x and \n",
    "𝑦\n",
    "y, whereas polynomial regression models a non-linear relationship.\n",
    "Model Flexibility: Polynomial regression can fit more complex data patterns by using higher-degree polynomials.\n",
    "Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Flexibility: Can model complex, non-linear relationships.\n",
    "Better Fit: Can capture curvature in the data that linear regression cannot.\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting: Higher-degree polynomials can overfit the data, capturing noise rather than the underlying trend.\n",
    "Interpretability: Coefficients of higher-degree terms are harder to interpret.\n",
    "Extrapolation: Polynomial models can produce unrealistic predictions outside the range of the data.\n",
    "When to Use Polynomial Regression:\n",
    "\n",
    "Non-linear Relationships: When the data shows a clear non-linear pattern that cannot be captured by a linear model.\n",
    "Curved Data Trends: When the relationship between predictors and the response variable has curvature.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
