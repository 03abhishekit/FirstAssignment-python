{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5334aed8-e7ef-4cdc-ab90-c7dbe3df7fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Ensemble techniques in machine learning combine the predictions of multiple models to produce a final prediction. \n",
    "This combination of models helps to improve the performance and robustness of the final model. \n",
    "Examples of ensemble techniques include Bagging, Boosting, and Stacking.\n",
    "\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "Ensemble techniques are used because they often produce more accurate and reliable predictions than any single model.\n",
    "They can reduce the variance (overfitting) and bias of the model and provide more robust performance.\n",
    "\n",
    "\n",
    "\n",
    "Q3. What is bagging?\n",
    "Bagging (Bootstrap Aggregating) is an ensemble technique that involves training multiple instances of the same model on different subsets of the data (created using bootstrapping) and then averaging their predictions. Random Forest is a popular example of a bagging method.\n",
    "\n",
    "Q4. What is boosting?\n",
    "Boosting is an ensemble technique that involves training models sequentially, where each subsequent model tries to correct the errors of the previous one. Examples include AdaBoost, Gradient Boosting, and XGBoost.\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "Benefits of ensemble techniques include:\n",
    "\n",
    "Improved prediction accuracy\n",
    "Reduction of overfitting\n",
    "Increased robustness to noise and variance\n",
    "Ability to capture a wider array of data patterns\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "Ensemble techniques are not always better than individual models. They generally perform better when individual models are weak learners but can sometimes perform worse if the individual models are very strong or if the ensemble method is not properly tuned.\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "In bootstrapping, the confidence interval can be calculated by repeatedly sampling from the dataset with replacement, computing the statistic of interest for each sample, and then finding the appropriate percentiles from the distribution of these statistics.\n",
    "\n",
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "Bootstrap works by repeatedly sampling from the data with replacement. The steps involved are:\n",
    "\n",
    "Randomly sample the data with replacement to create multiple bootstrap samples.\n",
    "Compute the statistic of interest for each bootstrap sample.\n",
    "Calculate the confidence interval from the distribution of the computed statistics.\n",
    "\n",
    "Q9. Bootstrap implementation to estimate the 95% confidence interval\n",
    "Let's demonstrate the implementation in a Jupyter notebook.\n",
    "\n",
    "Jupyter Notebook Implementation\n",
    "python\n",
    "Copy code\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import BaggingClassifier, AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "import joblib\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create and train an ensemble model using Bagging\n",
    "bagging_model = BaggingClassifier(base_estimator=RandomForestClassifier(), n_estimators=50, random_state=42)\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "# Bootstrap to estimate the 95% confidence interval for the mean height of trees\n",
    "# Assume we have sample data\n",
    "np.random.seed(42)\n",
    "sample_heights = np.random.normal(15, 2, 50)  # 50 sample tree heights with mean=15 and std=2\n",
    "\n",
    "# Function to calculate the mean\n",
    "def bootstrap_mean(data, n_bootstrap=1000, ci=95):\n",
    "    bootstrap_means = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        bootstrap_sample = resample(data)\n",
    "        bootstrap_means.append(np.mean(bootstrap_sample))\n",
    "    lower_bound = np.percentile(bootstrap_means, (100 - ci) / 2)\n",
    "    upper_bound = np.percentile(bootstrap_means, 100 - (100 - ci) / 2)\n",
    "    return np.mean(bootstrap_means), lower_bound, upper_bound\n",
    "\n",
    "mean_height, lower_ci, upper_ci = bootstrap_mean(sample_heights)\n",
    "print(f\"Mean height: {mean_height:.2f}\")\n",
    "print(f\"95% Confidence Interval: ({lower_ci:.2f}, {upper_ci:.2f})\")\n",
    "\n",
    "# Save the trained ensemble model\n",
    "joblib.dump(bagging_model, 'bagging_model.pkl')\n",
    "\n",
    "# Plot the distribution of bootstrap means\n",
    "bootstrap_means = [np.mean(resample(sample_heights)) for _ in range(1000)]\n",
    "plt.hist(bootstrap_means, bins=30, edgecolor='k', alpha=0.7)\n",
    "plt.axvline(lower_ci, color='r', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(upper_ci, color='r', linestyle='dashed', linewidth=1)\n",
    "plt.title('Bootstrap Distribution of the Mean Height')\n",
    "plt.xlabel('Mean Height')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "Explanation:\n",
    "Loading and Splitting the Dataset: We load the Iris dataset and split it into training and testing sets.\n",
    "Bagging Classifier: We create and train a BaggingClassifier using RandomForest as the base estimator.\n",
    "Model Evaluation: We predict the test set and evaluate the model using accuracy, precision, recall, and F1 score.\n",
    "Bootstrap Confidence Interval: We use bootstrapping to estimate the 95% confidence interval for the mean height of trees.\n",
    "Saving the Model: We save the trained model using joblib.\n",
    "Plotting: We plot the distribution of bootstrap means with the confidence interval marked."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
