{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba9900f-b3f4-4d6f-938d-fa98ddacfa20",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?\n",
    "\n",
    "R-squared (R²):\n",
    "Concept: R-squared is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in the regression model.\n",
    "\n",
    "Calculation: It is calculated as:\n",
    "\n",
    "𝑅2 = 1 − 𝑆𝑆res / 𝑆𝑆tot\n",
    " \n",
    "Where:\n",
    "𝑆𝑆res  is the sum of squares of residuals.\n",
    "𝑆𝑆tot  is the total sum of squares.\n",
    "Representation: An R² value of 1 indicates that the regression model perfectly fits the data, while an R² value of 0 indicates that the model does not explain any of the variability in the dependent variable.\n",
    "\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Adjusted R-squared:\n",
    "Concept: Adjusted R-squared adjusts the R-squared value based on the number of predictors in the model. It accounts for the model complexity by penalizing the addition of non-significant predictors.\n",
    "\n",
    "Calculation:\n",
    "\n",
    "𝑅adj2 = 1 −((1−𝑅2)(𝑛−1)/ 𝑛−𝑘−1)\n",
    "\n",
    "Where:\n",
    "n is the number of observations.\n",
    "k is the number of predictors.\n",
    "\n",
    "Difference from R-squared: While R-squared can only increase or remain the same when more predictors are added, adjusted R-squared can decrease if the added predictors do not improve the model.\n",
    "\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Appropriate Use of Adjusted R-squared:\n",
    "\n",
    "Adjusted R-squared is more appropriate when comparing models with a different number of predictors.\n",
    "\n",
    "It helps in identifying the model that explains the most variance while penalizing for unnecessary predictors, thus preventing overfitting.\n",
    "\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    "\n",
    "Calculation:\n",
    "𝑅𝑀𝑆𝐸 = Math.sqrt(1/𝑛 ∑ 𝑖=1𝑛 (𝑦𝑖 − 𝑦^𝑖)2)\n",
    "\n",
    "\n",
    " \n",
    "Representation: Measures the average magnitude of the error, giving higher weight to larger errors.\n",
    "\n",
    "MSE (Mean Squared Error):\n",
    "\n",
    "Calculation: \n",
    "𝑀𝑆𝐸 = 1/𝑛 ∑ 𝑖=1𝑛 (𝑦𝑖 − 𝑦^𝑖)2\n",
    "\n",
    "\n",
    "Representation: Measures the average squared difference between the observed and predicted values.\n",
    "MAE (Mean Absolute Error):\n",
    "\n",
    "Calculation:\n",
    "𝑀𝐴𝐸 = 1/𝑛 ∑𝑖=1𝑛 ∣ 𝑦𝑖 − 𝑦^𝑖 ∣\n",
    "\n",
    "Representation: Measures the average absolute difference between the observed and predicted values.\n",
    "\n",
    "\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis.\n",
    "Advantages:\n",
    "\n",
    "RMSE and MSE:\n",
    "Sensitive to large errors, which can be useful if larger errors are particularly undesirable.\n",
    "\n",
    "MAE:\n",
    "More robust to outliers, providing a more balanced view of model performance.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "RMSE and MSE:\n",
    "Can be overly sensitive to outliers.\n",
    "\n",
    "MAE:\n",
    "Less sensitive to large errors, which might be a disadvantage if capturing larger deviations is important.\n",
    "\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?\n",
    "Lasso Regularization:\n",
    "\n",
    "Concept: Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty equivalent to the absolute value of the magnitude of coefficients to the loss function.\n",
    "\n",
    "Lasso Cost Function = RSS + 𝜆 ∑𝑗=1𝑝 ∣ 𝛽𝑗 ∣\n",
    "\n",
    "Ridge Regularization:\n",
    "\n",
    "Concept: Ridge regression adds a penalty equivalent to the square of the magnitude of coefficients to the loss function.\n",
    "Ridge Cost Function\n",
    "=\n",
    "RSS\n",
    "+\n",
    "𝜆\n",
    "∑\n",
    "𝑗\n",
    "=\n",
    "1\n",
    "𝑝\n",
    "𝛽\n",
    "𝑗\n",
    "2\n",
    "Ridge Cost Function=RSS+λ \n",
    "j=1\n",
    "∑\n",
    "p\n",
    "​\n",
    " β \n",
    "j\n",
    "2\n",
    "​\n",
    " \n",
    "Differences:\n",
    "\n",
    "Lasso: Can shrink some coefficients to zero, effectively performing variable selection.\n",
    "Ridge: Shrinks coefficients but does not eliminate any, useful when all predictors are believed to have some effect.\n",
    "Appropriate Use:\n",
    "\n",
    "Lasso: When you expect that only a few predictors are important.\n",
    "Ridge: When you expect that all predictors contribute to some extent.\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate.\n",
    "Regularized Linear Models:\n",
    "\n",
    "Concept: Regularization techniques add a penalty to the loss function to constrain the size of the coefficients, thus preventing the model from fitting the noise in the training data.\n",
    "Example:\n",
    "\n",
    "In a dataset with many predictors, regularization can reduce the impact of less important variables, leading to a more generalizable model.\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis.\n",
    "Limitations:\n",
    "\n",
    "Interpretability: Regularization can make the model less interpretable by shrinking coefficients.\n",
    "Feature Scaling: Regularization requires feature scaling to ensure fair penalty across all predictors.\n",
    "Choice of Regularization Parameter: Selecting the regularization parameter (\n",
    "𝜆\n",
    "λ) is crucial and can be computationally intensive.\n",
    "Not Always Best Choice:\n",
    "\n",
    "When interpretability of the model is more important than prediction accuracy.\n",
    "When the dataset has very few predictors and overfitting is not a concern.\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics. Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better performer, and why? Are there any limitations to your choice of metric?\n",
    "Choosing the Better Performer:\n",
    "\n",
    "Model A (RMSE = 10): Indicates a higher penalty for larger errors.\n",
    "Model B (MAE = 8): Indicates a balanced view of errors without emphasizing larger ones.\n",
    "Choice of Metric:\n",
    "\n",
    "Context-Dependent: Choose RMSE if larger errors are critical, MAE if a balanced view is preferred.\n",
    "Limitations: RMSE can be skewed by outliers, while MAE might understate large deviations.\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?\n",
    "Choosing the Better Performer:\n",
    "\n",
    "Ridge Regularization (Model A): Better if you believe all predictors contribute to some extent.\n",
    "Lasso Regularization (Model B): Better if you want to perform variable selection.\n",
    "Trade-offs and Limitations:\n",
    "\n",
    "Ridge: Does not perform variable selection, less sparse models.\n",
    "Lasso: Can eliminate predictors, which is useful for simpler models but can be risky if important predictors are removed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
