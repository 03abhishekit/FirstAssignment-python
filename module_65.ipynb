{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3f4396-512f-4a00-962d-cdb455c3fcee",
   "metadata": {},
   "source": [
    "Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso Regression:\n",
    "Concept: Lasso (Least Absolute Shrinkage and Selection Operator) Regression is a type of linear regression that performs both variable selection\n",
    "and regularization. It adds a penalty equivalent to the absolute value of the magnitude of coefficients to the loss function.\n",
    "\n",
    "Cost Function:\n",
    "Cost¬†Function = RSS + ùúÜ‚àëùëó=1ùëù ‚à£ ùõΩùëó ‚à£\n",
    "\n",
    "Where \n",
    "Œª is the regularization parameter.\n",
    "Differences from Other Regression Techniques:\n",
    "\n",
    "Ordinary Least Squares (OLS): OLS minimizes the sum of squared residuals without any penalty.\n",
    "\n",
    "Ridge Regression: Adds an L2 penalty term (squared magnitude of coefficients), while Lasso adds an L1 penalty term (absolute value of coefficients).\n",
    "Elastic Net: Combines both L1 and L2 penalties.\n",
    "\n",
    "\n",
    "Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "Main Advantage:\n",
    "\n",
    "Feature Selection: Lasso Regression can shrink some coefficients to exactly zero, effectively performing feature selection by excluding less\n",
    "important features. This makes it useful for models with many features, reducing complexity and improving interpretability.\n",
    "\n",
    "\n",
    "Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "Interpreting Coefficients:\n",
    "\n",
    "Magnitude and Direction: The magnitude of the coefficients indicates the strength of the relationship between the predictor and the response variable,\n",
    "while the sign indicates the direction of the relationship.\n",
    "Zero Coefficients: Features with coefficients shrunk to zero by Lasso are considered unimportant for the prediction.\n",
    "\n",
    "\n",
    "Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "Tuning Parameters:\n",
    "\n",
    "Lambda (Œª): The regularization parameter controls the strength of the penalty.\n",
    "Higher Œª: Increases the penalty, leading to more coefficients being shrunk to zero (greater feature selection).\n",
    "Lower Œª: Decreases the penalty, making the model closer to OLS regression.\n",
    "Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Using Lasso for Non-Linear Problems:\n",
    "\n",
    "Basis Functions: Transform the features using basis functions (e.g., polynomial features, spline functions) to capture non-linearity.\n",
    "Example: Polynomial regression with Lasso can model non-linear relationships.\n",
    "python\n",
    "Copy code\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate polynomial features\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Apply Lasso Regression\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_poly, y)\n",
    "Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "Key Differences:\n",
    "\n",
    "Penalty Term:\n",
    "Ridge Regression: Uses L2 penalty (squared magnitude of coefficients).\n",
    "Lasso Regression: Uses L1 penalty (absolute value of coefficients).\n",
    "Feature Selection:\n",
    "Ridge Regression: Does not perform feature selection (coefficients are shrunk but not zero).\n",
    "Lasso Regression: Can shrink some coefficients to zero, performing feature selection.\n",
    "Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Handling Multicollinearity:\n",
    "\n",
    "Yes: Lasso Regression can handle multicollinearity by shrinking coefficients, potentially to zero. This reduces the impact of collinear features by effectively selecting only one feature among a group of correlated features.\n",
    "Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "Choosing Optimal Lambda (Œª):\n",
    "\n",
    "Cross-Validation: Use cross-validation to determine the best Œª by evaluating model performance on validation sets.\n",
    "Grid Search: Perform a grid search over a range of Œª values.\n",
    "python\n",
    "Copy code\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lasso = Lasso()\n",
    "params = {'alpha': [0.01, 0.1, 1, 10, 100]}\n",
    "grid_search = GridSearchCV(lasso, params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best lambda (alpha):\", grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
