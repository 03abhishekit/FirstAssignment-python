{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83236960-ebb8-4992-94a1-7a5e780500bf",
   "metadata": {},
   "source": [
    "Q.1.)What is the KNN algorithm?\n",
    "\n",
    "KNN (K-Nearest Neighbors) is a simple, instance-based learning algorithm used for classification and regression tasks. It works by finding the K closest data points in the feature space to a given query point and making predictions based on the majority class (for classification) or averaging the values (for regression) of those K points.\n",
    "\n",
    "Q.2.)How do you choose the value of K in KNN?\n",
    "\n",
    "The choice of K is crucial in KNN as it significantly affects the model's performance. A smaller K value leads to more complex decision boundaries, potentially resulting in overfitting, while a larger K value leads to smoother decision boundaries but may suffer from underfitting. Common methods for choosing K include cross-validation, grid search, or domain knowledge.\n",
    "\n",
    "\n",
    "Q.3.)What is the difference between KNN classifier and KNN regressor?\n",
    "\n",
    "KNN classifier predicts the class label of a query point based on the majority class of its K nearest neighbors, while KNN regressor predicts a continuous value by averaging the values of its K nearest neighbors.\n",
    "\n",
    "\n",
    "Q.4.)How do you measure the performance of KNN?\n",
    "\n",
    "Common performance metrics for KNN include accuracy, precision, recall, F1-score for classification tasks, and metrics like Mean Squared Error (MSE), Mean Absolute Error (MAE), or R-squared for regression tasks.\n",
    "\n",
    "\n",
    "Q.5.)What is the curse of dimensionality in KNN?\n",
    "\n",
    "The curse of dimensionality refers to the problem where the performance of KNN deteriorates as the number of dimensions (features) in the dataset increases. This is because in high-dimensional spaces, the notion of proximity becomes less meaningful, and the data points tend to spread out, making it difficult to identify meaningful nearest neighbors.\n",
    "\n",
    "\n",
    "Q.6.)How do you handle missing values in KNN?\n",
    "\n",
    "There are several approaches to handle missing values in KNN, including imputation (replacing missing values with estimated values), removing instances with missing values, or using algorithms that can handle missing values directly, such as KNN with distance weighting.\n",
    "\n",
    "\n",
    "Q.7.)Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
    "\n",
    "KNN classifier is suitable for classification problems where the output is categorical, while KNN regressor is suitable for regression problems where the output is continuous. The choice depends on the nature of the problem and the type of output variable.\n",
    "\n",
    "\n",
    "Q.8.)What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
    "\n",
    "Strengths: Simple to understand and implement, non-parametric (does not make assumptions about the underlying data distribution).\n",
    "Weaknesses: Sensitive to outliers, computationally expensive during prediction (especially for large datasets), requires careful selection of K.\n",
    "To address these weaknesses, techniques such as outlier detection and removal, dimensionality reduction, and efficient data structures (e.g., KD-trees) can be employed.\n",
    "\n",
    "\n",
    "Q.9.)What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space, while Manhattan distance is the sum of the absolute differences between the coordinates of two points. In KNN, Euclidean distance is often used for continuous features, while Manhattan distance may be more suitable for categorical or high-dimensional data.\n",
    "What is the role of feature scaling in KNN?\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the distance computation in KNN. Since KNN relies on the distance between data points, features with larger scales can dominate the distance calculation. Common techniques for feature scaling include Min-Max scaling or Z-score normalization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
